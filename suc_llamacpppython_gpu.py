# -*- coding: utf-8 -*-
"""suc_llamacpppython_GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GmUOLC2CgMrzauurpmBDJ5HgofCFN9M2
"""







!wget https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl

!pip install /content/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl

from llama_cpp import Llama

llm = Llama(
      model_path="/content/Llama-3.2-1B-Instruct-IQ3_M.gguf",
      n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
output = llm(
      "Q: Name the planets in the solar system? A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion
print(output)

!wget https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-IQ3_M.gguf

from llama_cpp import Llama

# إعداد النموذج
llm = Llama(
    model_path="/content/Llama-3.2-1B-Instruct-IQ3_M.gguf",
    # Uncomment the lines below if needed
    n_gpu_layers=-1, # Uncomment for GPU acceleration
    # seed=1337,       # Uncomment to set a specific seed
    # n_ctx=2048,      # Uncomment to increase the context window
)

# وظيفة للإجابة على الأسئلة
def ask_question(question):
    output = llm(
        f"Q: {question} A: ",  # يضيف السؤال إلى النص
        max_tokens=512,       # يحدد عدد التوكينات المخرجة
        stop=["Q:", "\n"],    # يتوقف عند بداية سؤال جديد أو سطر جديد
        echo=True             # إعادة طباعة السؤال في النتيجة
    )
    return output['choices'][0]['text'].strip()  # إعادة النص الناتج

# مثال على استخدام الوظيفة
question = "Who is Napoleon Bonaparte?"
answer = ask_question(question)
print(answer)

!wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf

from llama_cpp import Llama

# إعداد النموذج
llm = Llama(
    model_path="/content/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
    # Uncomment the lines below if needed
    n_gpu_layers=-1, # Uncomment for GPU acceleration
    # seed=1337,       # Uncomment to set a specific seed
    # n_ctx=2048,      # Uncomment to increase the context window
)

# وظيفة للإجابة على الأسئلة
def ask_question(question):
    output = llm(
        f"Q: {question} A: ",  # يضيف السؤال إلى النص
        max_tokens=512,       # يحدد عدد التوكينات المخرجة
        stop=["Q:", "\n"],    # يتوقف عند بداية سؤال جديد أو سطر جديد
        echo=True             # إعادة طباعة السؤال في النتيجة
    )
    return output['choices'][0]['text'].strip()  # إعادة النص الناتج

# مثال على استخدام الوظيفة
question = "Who is Napoleon Bonaparte?"
answer = ask_question(question)
print(answer)